{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "06face_identification_multi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandrakaku/ml0930/blob/master/06face_identification_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0QLLUCx-Mv7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VgBRJej427e"
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPbdfo2B427k"
      },
      "source": [
        "import pickle\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "import cv2\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from mtcnn import MTCNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXjK64Ss427o"
      },
      "source": [
        "folder_path = '/content/drive/My Drive/week10/face_detection'\n",
        "detector = MTCNN()\n",
        "feature_extractor = load_model(os.path.join(folder_path, 'facenet.h5'))\n",
        "model_age = load_model(os.path.join(folder_path, 'age100.h5'))\n",
        "model_gender = load_model(os.path.join(folder_path, 'gender100.h5'))\n",
        "model_emotion = load_model(os.path.join(folder_path, 'emotion48.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMpyxgBX427s"
      },
      "source": [
        "# read db\n",
        "with open(os.path.join(folder_path, 'db.pickle'), 'rb') as file:\n",
        "    db = pickle.load(file)\n",
        "embeddings = db['embeddings']\n",
        "names = db['names']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKh7cZvD427v"
      },
      "source": [
        "# detect face\n",
        "def detect_faces(img):\n",
        "    face_imgs = []\n",
        "    results = detector.detect_faces(img)\n",
        "    # extract the bounding box from the first face\n",
        "    print('# of faces: ', len(results))\n",
        "    for i in range(len(results)):\n",
        "        x1, y1, width, height = results[i]['box']\n",
        "        x2, y2 = x1 + width, y1 + height\n",
        "        patch = img[y1:y2, x1:x2] # crop face\n",
        "        face_imgs.append(patch)\n",
        "    return face_imgs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDHDN_sx427x"
      },
      "source": [
        "# Standardization\n",
        "def preprocess(imgs): \n",
        "    for i in range(imgs.shape[0]):\n",
        "        # standardization\n",
        "        img = imgs[i]\n",
        "        mean, std = img.mean(), img.std()\n",
        "        img = (img - mean) / std\n",
        "        imgs[i] = img\n",
        "    return imgs\n",
        "# Normalization\n",
        "def normalize(img):\n",
        "    return img / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUbSe1l0427z"
      },
      "source": [
        "def euclidean_distance(x, y):\n",
        "    sum_square = np.sum(np.square(x - y), keepdims=True)\n",
        "    return np.sqrt(sum_square)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ7iYh9R4271"
      },
      "source": [
        "def predict_age(img):\n",
        "    img_size = 100\n",
        "    img = normalize(img)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    model_input = np.zeros((1, img_size, img_size, 3))\n",
        "    model_input[0] = img\n",
        "    ages = model_age.predict(model_input)\n",
        "    print('age: ', ages[0])\n",
        "    return \n",
        "    \n",
        "def predict_emotion(img):\n",
        "    img_size = 48\n",
        "    cls_map = ['Angry', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral', 'Disgust']\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    img = normalize(img)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    model_input = np.zeros((1, img_size, img_size, 1))\n",
        "    model_input[0] = np.expand_dims(img, axis=-1) # 48,48 -> 48,48,1\n",
        "    emotions_pred = model_emotion.predict(model_input)\n",
        "    emotion_pred = emotions_pred[0]\n",
        "    emotion_name = cls_map[np.argmax(emotion_pred)]\n",
        "    print('emotion: ', emotion_name)\n",
        "    return\n",
        "def predict_gender(img):\n",
        "    img_size = 100\n",
        "    img = normalize(img)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    model_input = np.zeros((1, img_size, img_size, 3))\n",
        "    model_input[0] = img\n",
        "    genders = model_gender.predict(model_input)\n",
        "    gender = genders[0]\n",
        "    if gender > 0.5:\n",
        "        print('Male')\n",
        "    else:\n",
        "        print('Female')\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ovXLcl4274"
      },
      "source": [
        "def face_id(filename, IMG_SIZE=160):\n",
        "    raw_img = cv2.imread(os.path.join(folder_path, filename))[:,:,::-1]\n",
        "    faces = detect_faces(raw_img)\n",
        "    if len(faces) == 0:\n",
        "        print('No face')\n",
        "        return\n",
        "    else:\n",
        "        # get face embeddings\n",
        "        face = faces[0]\n",
        "        # More predictions\n",
        "        predict_age(face)\n",
        "        predict_emotion(face)\n",
        "        predict_gender(face)\n",
        "        # ID\n",
        "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
        "        model_input = np.zeros((1, IMG_SIZE, IMG_SIZE, 3))\n",
        "        model_input[0] = face\n",
        "        model_input = preprocess(model_input)\n",
        "        query_embeddings = feature_extractor.predict(model_input)\n",
        "        query_embedding = query_embeddings[0]\n",
        "        \n",
        "        # compute distance\n",
        "        distances = np.zeros((len(embeddings)))\n",
        "        for i, embed in enumerate(embeddings):\n",
        "            distance = euclidean_distance(embed, query_embedding)\n",
        "            distances[i] = distance\n",
        "\n",
        "        # find min distance    \n",
        "        idx_min = np.argmin(distances)\n",
        "        distance, name = distances[idx_min], names[idx_min]\n",
        "        print('name: ', name, ' distance: ',distance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCTM8GDk4276"
      },
      "source": [
        "path = 'w.jpg'\n",
        "face_id(path)\n",
        "plt.imshow(cv2.imread(os.path.join(folder_path, path))[:,:,::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gsZJamP4278"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}